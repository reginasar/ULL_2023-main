{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/reginasar/ULL_2023-main/hands-on/CAE_Star_Galaxy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bw7YySlOXq3H"
   },
   "source": [
    "#Unsupervised Star-Galaxy Separation with CAEs\n",
    "\n",
    "The goal of this tutorial is to build an unsupervised star-galaxy separation using Convolutional Variational Auto-Encoders. \n",
    "\n",
    "We train a convolutional autoencoder (CAE) with stamps of images of stars and galaxies (without any labels). We then explore the latent space learned by the CAE and check tha the CAE has learned that there were 2 different populations in the dataset.\n",
    "\n",
    "The dataset used for training is taken from COSMOS.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "#### Before we start, make sure to open this Colab notebook in \"PlayGround Mode\" (top left) and to change the Runtime type to GPU by navigating to the toolbar and clicking Runtime -> Change runtime type and then changing Hardware accelerator to GPU\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Hpv-XO8Sxgvi",
    "outputId": "38b90ba9-7ab5-4b13-dbea-7d974df2ed03"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-2911d52bc06a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtarfile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0murlretrieve\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_v2_behavior\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow_datasets\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtfds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import tarfile\n",
    "from urllib.request import urlretrieve\n",
    "import tensorflow.compat.v2 as tf\n",
    "tf.enable_v2_behavior()\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pdb\n",
    "!pip install umap-learn[plot]\n",
    "!pip install holoviews\n",
    "!pip install -U ipykernel\n",
    "import umap\n",
    "\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "\n",
    "tfk = tf.keras\n",
    "tfkl = tf.keras.layers\n",
    "tfpl = tfp.layers\n",
    "tfd = tfp.distributions\n",
    "\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u1PqZCKlVSMJ"
   },
   "source": [
    "## Data download and preparation\n",
    "\n",
    "Before mounting the drive click on [this folder](https://drive.google.com/drive/folders/1PcftgBzBySo1Ync-Wdsp9arTCJ_MfEPE?usp=sharing) and add it to your google drive by following these steps:\n",
    "\n",
    "*   Go to your drive \n",
    "*   Find shared folder (\"Shared with me\" link)\n",
    "*   Right click it\n",
    "*   Click Add to My Drive\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "HRoY9_k5FlN6",
    "outputId": "bd09df0d-ebd5-4c36-e1ca-9a738a413a6b"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import glob\n",
    "#glob.glob('/content/drive/MyDrive/*')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_3UL4LY330-k"
   },
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_dwLGPyqey_z"
   },
   "outputs": [],
   "source": [
    "pathinData=\"/content/drive/MyDrive/star-galaxy\"\n",
    "\n",
    "\n",
    "# donwload feature vector and labels\n",
    "X_ML = np.load(pathinData+'/x_train.npy')\n",
    "#morphological class\n",
    "Y_ML = np.load(pathinData+'/y_train.npy') \n",
    "#X_ML=X_ML-np.min(X_ML)\n",
    "#print(X_ML.min())\n",
    "#X_ML=X_ML/np.max(X_ML)\n",
    "X_ML=(X_ML[:,30-14:30+14,30-14:30+14,:])\n",
    "\n",
    "for i in range(len(X_ML)):\n",
    "  X_ML[i,:,:]=(X_ML[i,:,:]-np.mean(X_ML[i,:,:]))/np.std(X_ML[i,:,:])\n",
    "\n",
    "x_train=X_ML[0:int(len(X_ML)*3/5),:,:]\n",
    "x_val=X_ML[int(len(X_ML)*3/5):int(len(X_ML)*4/5),:,:]\n",
    "x_test=X_ML[int(len(X_ML)*4/5)::,:,:]\n",
    "\n",
    "y_train=Y_ML[0:int(len(X_ML)*3/5)]\n",
    "y_val=Y_ML[int(len(X_ML)*3/5):int(len(X_ML)*4/5)]\n",
    "y_test=Y_ML[int(len(X_ML)*4/5)::]\n",
    "\n",
    "#print((Y_ML==0).sum(), (Y_ML==1).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qpyzBpli33Xm"
   },
   "source": [
    "### Plot some random example\n",
    "Run multiple times to see more than one example. Class0 are stars, Class1 are galaxies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    },
    "colab_type": "code",
    "id": "xFw7AHgJGKkA",
    "outputId": "27581f6d-0344-4598-af78-71a62b09587d"
   },
   "outputs": [],
   "source": [
    "randomized_inds_train = np.random.permutation(len(x_train))\n",
    "\n",
    "fig = plt.figure()\n",
    "for i,j in zip(randomized_inds_train[0:4],range(4)):\n",
    "  ax = fig.add_subplot(2, 2, j+1)\n",
    "  im = ax.imshow(X_ML[i,:,:,0])\n",
    "  plt.title('$Class$='+str(y_train[i]))\n",
    "  fig.tight_layout() \n",
    "  fig.colorbar(im)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uSg9lTx76yuh"
   },
   "source": [
    "## Model Setup\n",
    "\n",
    "The following cells set up the model in TensorFlow. We have set up a very basic Convolutional Encoder-Decoder. It does not perform super well for this problem. I encourage you to try to modify this network to achieve better results. You should in particular try to reduce more the dimension of the bottleneck (end of encoder)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "897YRxETQMSE"
   },
   "outputs": [],
   "source": [
    "def pack_images(images, rows, cols):\n",
    "    \"\"\"Helper utility to make a field of images.\"\"\"\n",
    "    shape = tf.shape(images)\n",
    "    width = shape[-3]\n",
    "    height = shape[-2]\n",
    "    depth = shape[-1]\n",
    "    images = tf.reshape(images, (-1, width, height, depth))\n",
    "    batch = tf.shape(images)[0]\n",
    "    rows = tf.minimum(rows, batch)\n",
    "    cols = tf.minimum(batch // rows, cols)\n",
    "    images = images[:rows * cols]\n",
    "    images = tf.reshape(images, (rows, cols, width, height, depth))\n",
    "    images = tf.transpose(images, [0, 2, 1, 3, 4])\n",
    "    images = tf.reshape(images, [1, rows * width, cols * height, depth])\n",
    "    return images\n",
    "\n",
    "\n",
    "def image_tile_summary(name, tensor, rows=8, cols=8):\n",
    "    tf.summary.image(name, pack_images(tensor, rows, cols), max_outputs=1)\n",
    "\n",
    "\n",
    "input_shape = (28,28,1)\n",
    "encoded_size = 16  # THIS IS THE SIZE OF THE BOTTLENECK --> FEEL FREE TO CHANGE\n",
    "# AND EXPLORE\n",
    "base_depth = 32\n",
    "\n",
    "\n",
    "##################### CAE #####################\n",
    "\n",
    "encoder = tfk.Sequential([\n",
    "    tfkl.InputLayer(input_shape=input_shape),\n",
    "    #tfkl.Lambda(lambda x: tf.cast(x, tf.float32) - 0.5),\n",
    "    tfkl.Conv2D(base_depth, 5, strides=1,\n",
    "                padding='same', activation=tf.nn.leaky_relu),\n",
    "    tfkl.Conv2D(base_depth, 5, strides=2,\n",
    "                padding='same', activation=tf.nn.leaky_relu),\n",
    "    tfkl.Conv2D(2 * base_depth, 5, strides=1,\n",
    "                padding='same', activation=tf.nn.leaky_relu),\n",
    "    tfkl.Conv2D(2 * base_depth, 5, strides=2,\n",
    "                padding='same', activation=tf.nn.leaky_relu),\n",
    "    tfkl.Conv2D(4 * encoded_size, 7, strides=1,\n",
    "                padding='valid', activation=tf.nn.leaky_relu),\n",
    "    tfkl.Flatten(),\n",
    "    tfkl.Dense(encoded_size),\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "decoder = tfk.Sequential([\n",
    "    tfkl.InputLayer(input_shape=[encoded_size]),\n",
    "    tfkl.Reshape([1, 1, encoded_size]),\n",
    "    tfkl.Conv2DTranspose(2 * base_depth, 7, strides=1,\n",
    "                         padding='valid', activation=tf.nn.leaky_relu),\n",
    "    tfkl.Conv2DTranspose(2 * base_depth, 5, strides=1,\n",
    "                         padding='same', activation=tf.nn.leaky_relu),\n",
    "    tfkl.Conv2DTranspose(2 * base_depth, 5, strides=2,\n",
    "                         padding='same', activation=tf.nn.leaky_relu),\n",
    "    tfkl.Conv2DTranspose(base_depth, 5, strides=1,\n",
    "                         padding='same', activation=tf.nn.leaky_relu),\n",
    "    tfkl.Conv2DTranspose(base_depth, 5, strides=2,\n",
    "                         padding='same', activation=tf.nn.leaky_relu),\n",
    "    tfkl.Conv2DTranspose(base_depth, 5, strides=1,\n",
    "                         padding='same', activation=tf.nn.leaky_relu),\n",
    "    tfkl.Conv2D(filters=1, kernel_size=5, strides=1,\n",
    "                padding='same', activation=None),\n",
    "])\n",
    "\n",
    "\n",
    "cae = tfk.Model(inputs=encoder.inputs,outputs=decoder(encoder.outputs[0]))\n",
    "\n",
    "\n",
    "################## VAE #################\n",
    "\n",
    "prior = tfd.Independent(tfd.Normal(loc=tf.zeros(encoded_size), scale=1),\n",
    "                        reinterpreted_batch_ndims=1)\n",
    "\n",
    "vencoder = tfk.Sequential([\n",
    "    tfkl.InputLayer(input_shape=input_shape),\n",
    "    #tfkl.Lambda(lambda x: tf.cast(x, tf.float32) - 0.5),\n",
    "    tfkl.Conv2D(base_depth, 5, strides=1,\n",
    "                padding='same', activation=tf.nn.leaky_relu),\n",
    "    tfkl.Conv2D(base_depth, 5, strides=2,\n",
    "                padding='same', activation=tf.nn.leaky_relu),\n",
    "    tfkl.Conv2D(2 * base_depth, 5, strides=1,\n",
    "                padding='same', activation=tf.nn.leaky_relu),\n",
    "    tfkl.Conv2D(2 * base_depth, 5, strides=2,\n",
    "                padding='same', activation=tf.nn.leaky_relu),\n",
    "    tfkl.Conv2D(4 * encoded_size, 7, strides=1,\n",
    "                padding='valid', activation=tf.nn.leaky_relu),\n",
    "    tfkl.Flatten(),\n",
    "    tfkl.Dense(tfpl.MultivariateNormalTriL.params_size(encoded_size),\n",
    "               activation=None),\n",
    "    tfpl.MultivariateNormalTriL(\n",
    "        encoded_size,\n",
    "        activity_regularizer=tfpl.KLDivergenceRegularizer(prior,weight=0.6)),\n",
    "])\n",
    "\n",
    "vdecoder = tfk.Sequential([\n",
    "    tfkl.InputLayer(input_shape=[encoded_size]),\n",
    "    tfkl.Reshape([1, 1, encoded_size]),\n",
    "    tfkl.Conv2DTranspose(2 * base_depth, 7, strides=1,\n",
    "                         padding='valid', activation=tf.nn.leaky_relu),\n",
    "    tfkl.Conv2DTranspose(2 * base_depth, 5, strides=1,\n",
    "                         padding='same', activation=tf.nn.leaky_relu),\n",
    "    tfkl.Conv2DTranspose(2 * base_depth, 5, strides=2,\n",
    "                         padding='same', activation=tf.nn.leaky_relu),\n",
    "    tfkl.Conv2DTranspose(base_depth, 5, strides=1,\n",
    "                         padding='same', activation=tf.nn.leaky_relu),\n",
    "    tfkl.Conv2DTranspose(base_depth, 5, strides=2,\n",
    "                         padding='same', activation=tf.nn.leaky_relu),\n",
    "    tfkl.Conv2DTranspose(base_depth, 5, strides=1,\n",
    "                         padding='same', activation=tf.nn.leaky_relu),\n",
    "    tfkl.Conv2D(filters=1, kernel_size=5, strides=1,\n",
    "                padding='same', activation=None),\n",
    "    tfkl.Flatten(),\n",
    "    \n",
    "    tfkl.Dense(tfpl.IndependentNormal.params_size(input_shape),activation=None),\n",
    "    #tfpl.IndependentNormal(input_shape, tfd.Normal.mean),\n",
    "    tfpl.IndependentNormal(input_shape, tfd.Normal.sample),\n",
    "    #tfpl.IndependentBernoulli(input_shape, tfd.Bernoulli.logits),\n",
    "])\n",
    "\n",
    "vae = tfk.Model(inputs=vencoder.inputs, outputs=vdecoder(vencoder.outputs[0]))\n",
    "       \n",
    "\n",
    "       \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 799
    },
    "colab_type": "code",
    "id": "dMFExVtNiA-y",
    "outputId": "f9527a10-9a8a-41d8-a865-06ae7df6d0af"
   },
   "outputs": [],
   "source": [
    "encoder.summary()\n",
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MltdwyRD8O_c"
   },
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yGFe-4etRYFX"
   },
   "outputs": [],
   "source": [
    "#Define output path - This folder will contain the trained model\n",
    "pathout='star-galaxy/models/cae1'\n",
    "#pathout='star-galaxy/models/vae1'\n",
    "\n",
    "#Set RESET=True to delete all previous runs of the same model\n",
    "RESET=False\n",
    "if RESET:\n",
    "  tf.summary.FileWriterCache.clear()\n",
    "  os.system(\"rm -rf \"+ pathout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NyXCBjxYLxQP"
   },
   "source": [
    "Train first for 1000 steps. This is just a trick to run TensorBoard on the notebook and follow the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 697
    },
    "colab_type": "code",
    "id": "YEohb1jpRdwB",
    "outputId": "08406e98-f2ef-4a42-c422-57915a512c05"
   },
   "outputs": [],
   "source": [
    "cae.compile(optimizer=tf.optimizers.Adam(learning_rate=1e-3),\n",
    "            loss='mse')\n",
    "\n",
    "hist = cae.fit(x_train,x_train,epochs=20,validation_data=(x_val, x_val))\n",
    "\n",
    "\n",
    "#negloglik = lambda x, rv_x: -rv_x.log_prob(x)\n",
    "\n",
    "#vae.compile(optimizer=tf.optimizers.Adam(learning_rate=1e-3),\n",
    "#            loss=negloglik)\n",
    "\n",
    "#hist = vae.fit(x_train,x_train,epochs=20,validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.plot(hist.history['loss'])\n",
    "#plt.plot(hist.history['val_loss'],color='red')\n",
    "#plt.xlabel('Epochs')\n",
    "#plt.ylabel('Loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6C2ey9JtUjpb"
   },
   "source": [
    "## Predictions and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 577
    },
    "colab_type": "code",
    "id": "Z2T-duNTTi71",
    "outputId": "b629cac5-2c6b-4b54-c008-0bc2ae0683bb"
   },
   "outputs": [],
   "source": [
    "#predict of the first 10 examples\n",
    "pred = cae(x_test[0:10])\n",
    "\n",
    "fig = plt.figure()\n",
    "for i,j in zip(range(10),range(4)):\n",
    "  ax = fig.add_subplot(2, 2, j+1)\n",
    "  im = ax.imshow(x_val[i,:,:,0])\n",
    "  plt.title('$Class$='+str(y_val[i]))\n",
    "  fig.tight_layout() \n",
    "  fig.colorbar(im)\n",
    "\n",
    "fig = plt.figure()\n",
    "for i,j in zip(range(10),range(4)):\n",
    "  ax = fig.add_subplot(2, 2, j+1)\n",
    "  im = ax.imshow(pred[i,:,:,0])\n",
    "  plt.title('$Class$='+str(y_val[i]))\n",
    "  fig.tight_layout() \n",
    "  fig.colorbar(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#xhat = vae(x_val[0:10])\n",
    "#assert isinstance(xhat, tfd.Distribution)\n",
    "\n",
    "#print(x_test.shape)\n",
    "#pdb.set_trace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, let's generate ten never-before-seen galaxies and stars.\n",
    "#z = prior.sample(10)\n",
    "#xtilde = decoder(z)\n",
    "#assert isinstance(xtilde, tfd.Distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fig = plt.figure()\n",
    "#for i,j in zip(range(10),range(4)):\n",
    "#  ax = fig.add_subplot(2, 2, j+1)\n",
    "#  im = ax.imshow(xtilde.sample()[i,:,:,0])\n",
    "#  fig.tight_layout() \n",
    "#  fig.colorbar(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 326
    },
    "colab_type": "code",
    "id": "KiZd6NsZnvOq",
    "outputId": "f66e9f48-bc4d-45fb-ef3f-2f5d54f356ef"
   },
   "outputs": [],
   "source": [
    "# Now, let's explore the latent space\n",
    "z = encoder(x_test)\n",
    "print(\"shape of encoded space:\", z.shape)\n",
    "\n",
    "# since it is 16 dimensions, we are trying to visualize it with UMAP\n",
    "reducer = umap.UMAP()\n",
    "embedding = reducer.fit_transform(z)\n",
    "embedding.shape\n",
    "\n",
    "plt.xlabel(\"UMAP1\", fontsize=20)\n",
    "plt.ylabel(\"UMAP2\", fontsize=20)\n",
    "\n",
    "plt.scatter(embedding[y_test==1,1],embedding[y_test==1,0],color='blue',s=1,label='Galaxies')\n",
    "plt.scatter(embedding[y_test==0,1],embedding[y_test==0,0],color='red',s=1,label='Stars')\n",
    "plt.legend(fontsize=14)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NCyEH_j3ogoD"
   },
   "source": [
    "## Clustering with K-Means / GMMs\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "B1J4w5RpaA4W"
   },
   "source": [
    "Can you run a GMM in the encoded space? Look at the scikit learn documentation for GMMs. Use it to compute a classification accuracy. What is the fraction of stars classified as galaxies with this approach. The requirement for Euclid is less than 1%... \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "name": "CAE_Star_Galaxy.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
