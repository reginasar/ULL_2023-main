{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/reginasar/ULL_2023-main/blob/main/project/ULL23_MLproject.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XQFb4MCH9uSp"
   },
   "source": [
    "Do not forget to select a GPU Runtime before starting. \n",
    "\n",
    "The goal of this project is to perform simulation based inference with deep learning to constrain cosmological parameters. We will use the [CAMELS project](https://www.camel-simulations.org/). The Cosmology and Astrophysics with Machine Learning Simulations (CAMELS) is a suite of cosmological simulations with varying comsologies and subgrid physics, specially desgined for training Machine Learnning models. \n",
    "\n",
    "For this exercice we are going to use the [Multifield dataset](https://camels-multifield-dataset.readthedocs.io/en/latest/), which consists of a series of 2D Maps of different quantities (Stellar Mass, Total Mass, Gas Temperature, Velocity ...) for different cosmologies and subgrid parameters. \n",
    "\n",
    "**You will present the results in a pdf document together with a link to the code you used to generate the different plots.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gZwdb07rkvd5"
   },
   "source": [
    "# Connect to GCloud\n",
    "\n",
    "Just run these cells. Do not modify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eYE1KeFi0SGf"
   },
   "outputs": [],
   "source": [
    "from google.colab import auth\n",
    "auth.authenticate_user()\n",
    "!gcloud config set project candels-270716"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SdAp9WWy0p-r"
   },
   "outputs": [],
   "source": [
    "!echo \"deb http://packages.cloud.google.com/apt gcsfuse-bionic main\" > /etc/apt/sources.list.d/gcsfuse.list\n",
    "!curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -\n",
    "!apt -qq update\n",
    "!apt -qq install gcsfuse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-cUIWcrk0uZN"
   },
   "outputs": [],
   "source": [
    "!mkdir data\n",
    "!gcsfuse --implicit-dirs -o allow_other -file-mode=777 -dir-mode=777 camels data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T6wDbQGR7yem"
   },
   "source": [
    "# Import python modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aca2I0Z60zOm"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib \n",
    "import matplotlib.pyplot as plt\n",
    "from astropy.visualization import astropy_mpl_style\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import sklearn\n",
    "\n",
    "tfd = tfp.distributions\n",
    "tfpl = tfp.layers\n",
    "tfk = tf.keras\n",
    "tfkl = tf.keras.layers\n",
    "plt.style.use(astropy_mpl_style)\n",
    "print(tfp.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pv9qDIW_71tx"
   },
   "source": [
    "#Load Maps and Corresponding parameters\n",
    "The maps can be changed.\n",
    "\n",
    "The models can be: \n",
    " - IllustrisTNG\n",
    " -  SIMBA\n",
    "\n",
    "The map types can be:\n",
    "-  T (Gas Temperature)\n",
    "-  Mstar (stellar mass)\n",
    "- Z (Metallicity) \n",
    "-  Mgas (Gas Mass)\n",
    "- Vgas (Gas Velocity)\n",
    "- Mtot (total mass) etc..\n",
    "\n",
    "THe list of available files is in /content/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GT8s4KZ705Rq"
   },
   "outputs": [],
   "source": [
    "## This function loads the maps and corresponding parameters\n",
    "\n",
    "def load_maps(model,maptype):\n",
    "\n",
    "  fmaps = '/content/data/Maps_'+maptype+'_'+model+'_LH_z=0.00.npy'\n",
    "  maps  = np.load(fmaps)\n",
    "  fparams = '/content/data/params_'+model+'.txt'\n",
    "  params  = np.loadtxt(fparams)\n",
    "  return maps,params\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-dUWy_D62bfH"
   },
   "outputs": [],
   "source": [
    "# loads the temperature map of Illustris TNG - can take a while\n",
    "maps, params = load_maps('IllustrisTNG','T')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lZP4c__TelPB"
   },
   "source": [
    "The following cell shows a random example of the loaded map together with the corresponding parameters. You can run it several times. The goal is to design a deep learning model to infer cosmological and astrophysical parameters using the different maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7xIeiIgh1kQh"
   },
   "outputs": [],
   "source": [
    "from random import randrange\n",
    "map_number = randrange(15000)\n",
    "plt.imshow(np.log10(maps[map_number]),cmap=plt.get_cmap('binary_r'), origin='lower', interpolation='bicubic')\n",
    "plt.show()\n",
    "params_map = params[map_number//15] ## This is how params and maps are connected. \n",
    "print('Value of the parameters for this map')\n",
    "print('Omega_m: %.5f'%params_map[0])\n",
    "print('sigma_8: %.5f'%params_map[1])\n",
    "print('A_SN1:   %.5f'%params_map[2])\n",
    "print('A_AGN1:  %.5f'%params_map[3])\n",
    "print('A_SN2:   %.5f'%params_map[4])\n",
    "print('A_AGN2:  %.5f'%params_map[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q2DIjO1tgQbt"
   },
   "source": [
    "# Exercice 1: \n",
    "Create a Convolutional Neural Network that takes as input the map of gas temperature (T) from the IllustrisTNG model loaded in the and estimates $\\Omega_m$, the Dark Matter Density. The output of the Neural Network should be a Gaussian Probability Density Function. You can also experiment with multimodal Gaussians (see build_model() function defined below).\n",
    "\n",
    "You will produce:\n",
    "- A summary of the architecture used. \n",
    "\n",
    "     --> For this see plot_model() function from keras.utils.vis_utils and visualkeras libraries.\n",
    "     \n",
    "     \n",
    "- A plot of the learning history including the trainng and the validation sets.\n",
    "\n",
    "     --> You can use ideas from previous exercises (epochs vs. Loss).\n",
    "     \n",
    "     \n",
    "- A plot of input vs. output on the test set, including error bars. You will use the mean of posterior as output.\n",
    "\n",
    "     --> Real vs mean prediction for the test set, plot the identity function to compare.\n",
    "     \n",
    "     Do p = model1(Xtest) # to get a pdf for each test item. Do p.mean() to get the mean prediction for each test item and p.std() for the standard deviation.\n",
    "     \n",
    "     \n",
    "- Ten example plots of posterior distributions, together with the ground truth.\n",
    "\n",
    "     --> As before, do p.logprob(x).numpy() to get the  posterior logprobability of given x. To get the posterior probability just take np.exp() of the previous value.\n",
    "\n",
    "\n",
    "- 1-sigma value of the posterior versus the true error for the test set. Does the posterior properly capture uncertainty? Comment.\n",
    "\n",
    "     ---> This means one standard deviation vs (Y_true - Y_pred), we can consider Y_pred as the mean of the posterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to generate training, validation and test sets. Takes as input parameters\n",
    "# the sizes of the differnet datasets. If you use colab, limit to ~3000 samples \n",
    "# for training to avoid memory issues\n",
    "\n",
    "def train_test(ntrain,nval,ntest):\n",
    "  Y=[]\n",
    "  for i in range(maps.shape[0]):\n",
    "    Y.append(params[i//15][0])\n",
    "\n",
    "\n",
    "  X, Y = sklearn.utils.shuffle(maps, Y)\n",
    "\n",
    "  Xtrain=X[0:ntrain]\n",
    "  Ytrain = Y[0:ntrain]\n",
    "\n",
    "  Xval = X[ntrain:ntrain+nval]\n",
    "  Yval = Y[ntrain:ntrain+nval]\n",
    "\n",
    "  Xtest = X[ntrain+nval:ntrain+nval+ntest]\n",
    "  Ytest = Y[ntrain+nval:ntrain+nval+ntest]\n",
    "\n",
    "  return Xtrain,Ytrain,Xval,Yval,Xtest,Ytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define here the function build_model(nfilters, num_components, input_shape, output_shape)\n",
    "# where num_components is the number of output probabilities you'll have. Remember that the \n",
    "# final layer should output a posterior probability (a pdf, not just a number per item), like\n",
    "# the layer tfpl.MixtureNormal(). this function should also compile the model\n",
    "# You can use class 3 exercise as an example, check the input shapes are correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain, Ytrain, Xval, Yval, Xtest, Ytest = train_test(3000, 300, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = build_model(16,1,256,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist1 = model1.fit(np.expand_dims(Xtrain,axis=3),np.array(Ytrain),epochs = 5, validation_data=(np.expand_dims(Xval,axis=3),np.array(Yval)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = model1(Xtest) #get a pdf for each test item\n",
    "# do p.mean() to get the mean prediction for each test item\n",
    "# and p.std() for the standard deviation\n",
    "# do p.logprob(x).numpy() to get the  posterior logprobability of given x\n",
    "# to get the posterior probability just take np.exp() of the previous value\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yd0a5UgqigwH"
   },
   "source": [
    "# Exercice 2: \n",
    "Test the deep learning model on the same map, produced now with the SIMBA cosmological model. \n",
    "\n",
    "Tip: if you run out of memory, save the weights of the previous training (model.save_weights) in your GDrive , restart the notebook and load. GDrive can be mounted with the left menu.\n",
    "\n",
    "- Produce a scatter plot, including error bars, of input Omega_m versus output Omega_m. Comment.\n",
    "- Plot ten example plots of posterior distributions\n",
    "- Plot the 1-sigma value of the posterior versus the true error. Does the posterior properly capture uncertainty? Comment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KzrpkFoIpIC8"
   },
   "source": [
    "# Exercice 3: \n",
    "Repeat the steps above with an astrophysics parameter, e.g A_AGN_1, which parametrizes the type of AGN feedback. Compare and comment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8-zj5Io4vlkg"
   },
   "source": [
    "# Exercice 4 [Optional]\n",
    "\n",
    "Desing a domain adaptation technique so that the model trained on TNG can work in SIMBA."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPm7lhQcGy8sp1Ra3HnR7Mc",
   "collapsed_sections": [],
   "include_colab_link": true,
   "mount_file_id": "1Uud2sY36EWxfZpiPCT483zjZfm0OL6uw",
   "name": "ULL22_MLproject.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
